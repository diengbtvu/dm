{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d7625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "labels = []\n",
    "def load_images_from_folder(folder):\n",
    "    data = []   \n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(folder, filename))\n",
    "            data.append(img)\n",
    "\n",
    "            # for printing results\n",
    "            labels.append(filename)\n",
    "    return data\n",
    "\n",
    "dog_images = load_images_from_folder('training/dog')\n",
    "cat_images = load_images_from_folder('training/cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddb239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d5bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 224, 224, 3)\n",
      "(10, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dog_images = np.array(dog_images)\n",
    "cat_images = np.array(cat_images)\n",
    "\n",
    "print(dog_images.shape)\n",
    "print(cat_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354c571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6044004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quangvinh/miniconda3/envs/torchenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# use it as a magic tool\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# CLIP\n",
    "def magic_function(images):\n",
    "    inputs = clip_processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    return features.numpy()\n",
    "\n",
    "dog_images_features = magic_function(list(dog_images))\n",
    "cat_images_features = magic_function(list(cat_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c48978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 512)\n",
      "(10, 512)\n"
     ]
    }
   ],
   "source": [
    "print(dog_images_features.shape)\n",
    "print(cat_images_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "748d5667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 512)\n"
     ]
    }
   ],
   "source": [
    "merged_features = np.concatenate((dog_images_features, cat_images_features), axis=0)\n",
    "print(merged_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc039d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "938cf475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Cluster 1: dog.1.jpg\n",
      "Cluster 1: dog.0.jpg\n",
      "Cluster 1: dog.2.jpg\n",
      "Cluster 1: dog.3.jpg\n",
      "Cluster 0: dog.7.jpg\n",
      "Cluster 1: dog.6.jpg\n",
      "Cluster 1: dog.4.jpg\n",
      "Cluster 1: dog.5.jpg\n",
      "Cluster 1: dog.8.jpg\n",
      "Cluster 1: dog.9.jpg\n",
      "Cluster 0: cat.6.jpg\n",
      "Cluster 0: cat.7.jpg\n",
      "Cluster 0: cat.5.jpg\n",
      "Cluster 0: cat.4.jpg\n",
      "Cluster 0: cat.0.jpg\n",
      "Cluster 0: cat.1.jpg\n",
      "Cluster 0: cat.3.jpg\n",
      "Cluster 0: cat.2.jpg\n",
      "Cluster 0: cat.9.jpg\n",
      "Cluster 0: cat.8.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(merged_features)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "for real_label, predicted_label in zip(labels, kmeans.labels_):\n",
    "    print(f\"Cluster {predicted_label}: {real_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a0d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
